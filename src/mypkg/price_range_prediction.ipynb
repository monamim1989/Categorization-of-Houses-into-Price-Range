{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Categorization-of-Houses-into-Price-Range\n",
    "__Categorization of Houses into Different Price Range using ML Algorithms from American Housing Survey 2017 Dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The main goal of this project is to predict the range of selling price of house with a high degree of predictive accuracy using various Machine Learning methods. Given house sale data or explanatory variable such as number of bedrooms, number of bathrooms in unit, housing cost, annual commuting cost etc, the model is built. Next, the model is evaluated with respect to test data, and plot the prediction and coefficients.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For my project, I have prepared two types of file for the same code - one .py and other .ipynb. The .py version is for testing using pytest. I am applying different machine learning algorithms and using a big dataset. Therefore, my .ipynb file became too large (around 90MB) which cannot be uploaded in github repo as it is. Therefore, I prepared a PDF copy of .ipynb file with all outputs that got generated, so that outputs of program are visible. Also, I cleared all outputs for .ipynb file and uploaded that as well. All the relevant documents along with the .ipynb with all generated outputs is present in google drive - https://drive.google.com/drive/u/0/folders/1Or1xQ5GVPU1sCB3hY7V5pAKYYp-aP2Nd__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import re\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "#pd.set_option('display.max_columns', 1000)\n",
    "import math\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# Learning Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__I am using American Housing Survey 2017 data https://www.census.gov/programs-surveys/ahs/data/2017/ahs-2017-public-use-file--puf-/ahs-2017-national-public-use-file--puf-.html (household.csv in AHS 2017 National PUF v3.0 CSV.zip). Since the dataset is very big (441 MB), I am just providing the link. It could not be uploaded in github repo. There is another csv file called AHSDICT_15NOV19_21_17_31_97_S.csv that consist of the mapping information of each feature name to their actual meaning and data type information. This file is already present in github repo. In the AHS microdata, the basic unit is an individual housing unit. Each record shows most of the information associated with a specific housing unit or individual, except for data items that could be used to personally identify that housing unit or individual. The dataset comprises of housing data features like TOTROOMS(Number of rooms in unit), PERPOVLVL(Household income as percent of poverty threshold (rounded)), COMCOST(Total annual commuting cost), JBATHROOMS(Number of bathrooms in unit), UNITSF(Square footage of unit), JGARAGE(Flag indicating unit has a garage or carport), JFIREPLACE(Flag indicating unit has a useable fireplace)  etc., and target column as MARKETVAL(Current market value of unit) to evaluate model and also check which amongst all features is the most correlated feature for price predication.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"household.csv\")\n",
    "headings = pd.read_csv(\"AHSDICT_15NOV19_21_17_31_97_S.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataset into a format that can be processed further\n",
    "col_to_check = data.columns\n",
    "data[col_to_check] = data[col_to_check].replace({'\\'':''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column CONTROL is not relevant to our problem, so we can remove that \n",
    "col_to_remo = ['CONTROL']\n",
    "data = data.drop(col_to_remo, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all Not Applicable/No Response values with Nan for further processing\n",
    "L = ['-6', -6, '-9', -9, 'M', 'N']\n",
    "data = data.replace(L, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of non relevant values\n",
    "for c in list(data.columns): \n",
    "    nan = (len(data) - data[c].count())/(len(data))\n",
    "    if nan >= 0.85:\n",
    "        del data[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "data['MARKETVAL'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[pd.notnull(data['MARKETVAL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = data[ data['MARKETVAL'] < 50000 ].index\n",
    "# Delete these row indexes from dataFrame\n",
    "data.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of data\n",
    "plt.hist(data['MARKETVAL'], bins = int(180/5), color = 'blue', edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into numerical and categorical features\n",
    "col_o = list(data.columns)\n",
    "numeric = []\n",
    "categorical = []\n",
    "e = []\n",
    "for c in col_o:\n",
    "    j = 0\n",
    "    if c[0]=='J':\n",
    "        j = 1\n",
    "        c = c[1:]\n",
    "    h = headings.loc[headings['Variable']== c]['TYPE'].tolist()\n",
    "    if h != []:\n",
    "        if (h[0] == 'Character'):\n",
    "            if j == 0:\n",
    "                categorical.append(c)\n",
    "            elif j == 1:\n",
    "                categorical.append('J' + c)\n",
    "        elif (h[0] == 'Numeric'):\n",
    "            if j == 0:\n",
    "                numeric.append(c)\n",
    "            elif j == 1:\n",
    "                numeric.append('J' + c)\n",
    "        else:\n",
    "            if j == 0:\n",
    "                e.append(c)\n",
    "            elif j == 1:\n",
    "                e.append('J' + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data_numeric which only has numerical features\n",
    "data_numeric = data.drop(categorical, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of all NaN entries in data_numeric\n",
    "data_numeric = data_numeric.fillna(data_numeric.mean())\n",
    "for i in numeric:\n",
    "    if math.isnan(float(data_numeric[i].mean())):\n",
    "        # drop the unnecessary columns\n",
    "        print(\"Dropping the Column: \", i)\n",
    "        data_numeric = data_numeric.drop(i, axis = 1)\n",
    "    else:\n",
    "        data_numeric[i] = data_numeric[i].fillna(data_numeric[i].mean())\n",
    "\n",
    "print(data_numeric.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data_categorical which only has categorical features\n",
    "data_categorical = data.drop(numeric, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of all NaN entries in data_catagorical\n",
    "for i in categorical:\n",
    "    # dict to store counts of each unique value occurring for each feature\n",
    "    freq = {}\n",
    "    for j in data_categorical[i]:\n",
    "        if (j in freq): \n",
    "            freq[j] += 1\n",
    "        else: \n",
    "            freq[j] = 1\n",
    "    freq_sorted = sorted(freq, key=freq.get, reverse=True)\n",
    "    \n",
    "    # if the most frequent value is Nan\n",
    "    if math.isnan(float(freq_sorted[0])):\n",
    "        # if Nan is not the only value for that feature, then use the next most frequent value to replace Nan\n",
    "        if len(freq_sorted) > 1:\n",
    "            mode_val = freq_sorted[1]\n",
    "            data_categorical[i] = data_categorical[i].fillna(mode_val)\n",
    "        # if Nan is the only value for that feature, then drop the column\n",
    "        else: \n",
    "            # drop the unnecessary columns\n",
    "            print(\"Dropping the Column: \", i)\n",
    "            data_categorical = data_categorical.drop(i, axis = 1)\n",
    "    else:\n",
    "        mode_val = freq_sorted[0]\n",
    "        data_categorical[i] = data_categorical[i].fillna(mode_val)\n",
    "        \n",
    "print(data_categorical.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate numerical and categorical data\n",
    "clean_data = pd.concat([data_numeric, data_categorical], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns after concatenation\n",
    "clean_data = clean_data.iloc[:,~clean_data.columns.duplicated()]\n",
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "__Correlation matrix to check which amongst all features is the most correlated feature for price prediction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=clean_data.corr()\n",
    "corr_matrix[\"MARKETVAL\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The dataset was cleaned to make it free from erroneous or irrelevant data. By filling up missing values, removing rows and reducing data size, the final dataset was (36358 rows X 1007 columns).__\n",
    "\n",
    "# Dataset Split\n",
    "\n",
    "__Now the Test Data and Train Data will be separated. Will keep 30% of the data for Testing purpose and rest for training purpose.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the target\n",
    "y = clean_data['MARKETVAL']\n",
    "\n",
    "# Separating out the features\n",
    "x = clean_data.drop('MARKETVAL', axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into numerical and categorical features\n",
    "col_o = list(x.columns)\n",
    "numeric = []\n",
    "categorical = []\n",
    "e = []\n",
    "for c in col_o:\n",
    "    j = 0\n",
    "    if c[0]=='J':\n",
    "        j = 1\n",
    "        c = c[1:]\n",
    "    h = headings.loc[headings['Variable']== c]['TYPE'].tolist()\n",
    "    if h != []:\n",
    "        if (h[0] == 'Character'):\n",
    "            if j == 0:\n",
    "                categorical.append(c)\n",
    "            elif j == 1:\n",
    "                categorical.append('J' + c)\n",
    "        elif (h[0] == 'Numeric'):\n",
    "            if j == 0:\n",
    "                numeric.append(c)\n",
    "            elif j == 1:\n",
    "                numeric.append('J' + c)\n",
    "        else:\n",
    "            if j == 0:\n",
    "                e.append(c)\n",
    "            elif j == 1:\n",
    "                e.append('J' + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Data for numeric features\n",
    "X_train_numeric = X_train.drop(categorical, axis = 1)\n",
    "X_test_numeric = X_test.drop(categorical, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now, the data will be standardized and normalyzed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar Transform the Numeric Variable\n",
    "scaler = MinMaxScaler()#StandardScaler()\n",
    "scaler.fit(X_train_numeric)\n",
    "\n",
    "# Apply transform to both the training set and the test set. \n",
    "X_train_numeric_trans = pd.DataFrame(scaler.transform(X_train_numeric))\n",
    "X_test_numeric_trans = pd.DataFrame(scaler.transform(X_test_numeric))\n",
    "\n",
    "X_train_numeric_trans.columns = X_train_numeric.columns\n",
    "X_test_numeric_trans.columns = X_test_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Data for catagorical features\n",
    "X_train_categorical = X_train.drop(numeric, axis = 1)\n",
    "X_test_categorical = X_test.drop(numeric, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index to avoid nan on concatenation\n",
    "X_train_numeric_trans.reset_index(drop=True, inplace=True)\n",
    "X_train_categorical.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_test_numeric_trans.reset_index(drop=True, inplace=True)\n",
    "X_test_categorical.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating numeric and catagorical features\n",
    "X_train = pd.concat([X_train_numeric_trans, X_train_categorical], axis=1, sort=False)\n",
    "X_test = pd.concat([X_test_numeric_trans, X_test_categorical], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns after concatenation\n",
    "X_train = X_train.iloc[:,~X_train.columns.duplicated()]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns after concatenation\n",
    "X_test = X_test.iloc[:,~X_test.columns.duplicated()]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Range Encoding\n",
    "__Since, I want to classify houses into different price ranges, I will need to perform feature encoding for various price ranges.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final formatting before applying algorithms\n",
    "y_train = pd.DataFrame(y_train) \n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_train_int = y_train.astype(int)\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "y_test_int = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign code for different price ranges\n",
    "def price_range(price):\n",
    "    if price < 100000:\n",
    "        return 1\n",
    "    elif price >= 100000 and price < 250000:\n",
    "        return 2\n",
    "    elif price >= 250000 and price < 500000:\n",
    "        return 3\n",
    "    elif price >= 500000 and price < 750000:\n",
    "        return 4\n",
    "    elif price >= 750000 and price < 1000000:\n",
    "        return 5\n",
    "    elif price >= 1000000 and price < 1250000:\n",
    "        return 6\n",
    "    elif price >= 1250000:\n",
    "        return 7\n",
    "    else:\n",
    "        print(price)\n",
    "        return 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the price range encoded field in y dataset\n",
    "y_train['MARKETVAL'] = y_train_int['MARKETVAL'].apply(price_range)\n",
    "y_test['MARKETVAL'] = y_test_int['MARKETVAL'].apply(price_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__From below histograms it can be seen that most houses fall in the MARKETVAL range of 100000 to 250000__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of data in y training dataset\n",
    "plt.hist(y_train['MARKETVAL'], bins = int(180/5), color = 'blue', edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of data in y testing dataset\n",
    "plt.hist(y_test['MARKETVAL'], bins = int(180/5), color = 'blue', edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding to represent categorical variables as binary vectors\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train_le = le.transform(y_train['MARKETVAL'])#.reshape(-1, 1)\n",
    "#y_test_le = le.transform(y_train['MARKETVAL'])#.reshape(-1, 1)\n",
    "\n",
    "oh = OneHotEncoder(sparse=False)\n",
    "y_train_le = y_train_le.reshape(len(y_train_le), 1)\n",
    "oh.fit(y_train_le)\n",
    "\n",
    "y_train_oh = oh.transform(y_train_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "__The function accuracy is used to calculate accuracy scores for both training and testing dataset for different ML models. The function train_model is used to train and fit the data for different classifier models.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy score for training and testing datasets\n",
    "def accuracy(X_train, X_test, y_train, y_test, model): \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train.values, y_pred_train)\n",
    "    print(f\"Train accuracy: {train_accuracy:0.2%}\")\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test.values, y_pred_test)\n",
    "    print(f\"Test accuracy: {test_accuracy:0.2%}\")\n",
    "    \n",
    "    # For comparison of models later\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train data based on different classifiers\n",
    "def train_model(X_train, X_test, classifier, **kwargs):\n",
    "    model = classifier(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Implemented\n",
    "__In this project, my aim was to implement algorithms which will be able to learn and classify the new observations to correct house price ranges. I decided to use below machine learning algorithms for the same-<br >\n",
    "•\tRandom Forest (RandomForestClassifier)<br >\n",
    "•\tLogistic Regression (LogisticRegression)<br >\n",
    "•\tK-Nearest Neighbor (KNeighborsClassifier)<br >\n",
    "•\tDecision Tree (DecisionTreeClassifier)__<br >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy for all models for comparison later\n",
    "accuracy_val = []\n",
    "# List of Algorithms Mames\n",
    "classifiers = ['Random Forest', 'Logistic Regression', 'Knn (7 Neighbors)', 'Decision Tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "__The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees, this model uses two key concepts that gives it the name random:<br >\n",
    "•\tRandom sampling of training data points when building trees<br >\n",
    "•\tRandom subsets of features considered when splitting nodes<br >\n",
    "The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier\n",
    "model = train_model(X_train, y_train_oh, RandomForestClassifier, n_estimators=200, random_state=20)\n",
    "test_accuracy_val = accuracy(X_train, X_test, y_train, y_test, model)\n",
    "accuracy_val.append(test_accuracy_val)\n",
    "# Top 10 features that determine price\n",
    "pd.Series(model.feature_importances_, x.columns).sort_values(ascending=True).nlargest(10).plot.barh(align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results: With RandomForestClassifier, the accuracy score were as below:<br >\n",
    "Training Accuracy – 100.00%<br >\n",
    "Testing Accuracy – 55.86%__ <br >\n",
    "\n",
    "__I also plotted a bar graph above representing the top 10 features based on their importance in determining the house price range.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "__Logistic regression is one of the most fundamental and widely used Machine Learning Algorithms. Logistic regression is not a regression algorithm but a probabilistic classification model. Multi class classification is implemented by training multiple logistic regression classifiers, one for each of the K classes in the training dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression\n",
    "model = train_model(X_train, y_train, LogisticRegression,solver='lbfgs')\n",
    "test_accuracy_val = accuracy(X_train, X_test, y_train, y_test, model)\n",
    "accuracy_val.append(test_accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results: With LogisticRegression, the accuracy score were as below:<br >\n",
    "Training Accuracy – 47.08%<br >\n",
    "Testing Accuracy – 46.68%__<br >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor\n",
    "__KNN or k-nearest neighbours is the simplest classification algorithm. This classification algorithm does not depend on the structure of the data. Whenever a new example is encountered, its k nearest neighbours from the training data are examined. Distance between two examples can be the euclidean distance between their feature vectors. The majority class among the k nearest neighbours is taken to be the class for the encountered example.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using kNN Classifier\n",
    "model = train_model(X_train, y_train, KNeighborsClassifier, n_neighbors=7)\n",
    "test_accuracy_val = accuracy(X_train, X_test, y_train, y_test, model)\n",
    "accuracy_val.append(test_accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results: With KNeighborsClassifier, the accuracy score were as below:<br >\n",
    "Training Accuracy – 60.04%<br >\n",
    "Testing Accuracy – 46.89%__<br >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "__Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree. On the root and each of the internal nodes, a question is posed and the data on that node is further split into separate records that have different characteristics. The leaves of the tree refer to the classes in which the dataset is split.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Decision Tree Classifier\n",
    "model = train_model(X_train, y_train, DecisionTreeClassifier, max_depth=8)\n",
    "test_accuracy_val = accuracy(X_train, X_test, y_train, y_test, model)\n",
    "accuracy_val.append(test_accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results: With DecisionTreeClassifier, the accuracy score were as below:<br >\n",
    "Training Accuracy – 65.47%<br >\n",
    "Testing Accuracy – 59.89%__<br >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The purpose of this project was correlate and compare the above mentioned ML algorithms in order to check their performances.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from accuracy results\n",
    "summary = pd.DataFrame({'Test Accuracy':accuracy_val}, index=classifiers)       \n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For this particular problem, the algorithm with best accuracy value is DecisionTreeClassifier with test accuracy score of 59.89% and therefore it can be considered as a good classifier algorithm for house price range prediction problem. Also, the RandomForestClassifier is close enough with 55.86% accuracy score. Since the accuracy values are not very high, I have tried tuning each algorithm with different hyper-parameter values and finally kept the best results that I could get. In this project we can say that in machine learning problems data processing and tuning makes the model more accurate and efficient compare to non processed data. It also makes simple models quite accurate.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
